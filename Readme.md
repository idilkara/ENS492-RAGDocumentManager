This is to run a llm server on port 11434 using docker

There is a shell script for this that you can modify or just run your model exposing to the specified ports! :)

example command to run: 

    bash ollama-config.sh llama3

dont forget to 
    docker stop ollama

