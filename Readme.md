This is to run a llm server on port 30000 using docker

There is a shell script for this that you can modify or just run your model! :)

    ollama-config.sh 